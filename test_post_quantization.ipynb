{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ref https://github.com/hkproj/quantization-notes/tree/main\n",
    "#Ref https://pytorch.org/docs/stable/quantization-support.html\n",
    "import torch\n",
    "import torchvision.datasets as datasets \n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp_delme.p\")\n",
    "    model_size = os.path.getsize(\"temp_delme.p\")/1e3\n",
    "    print('Size (KB):', model_size)\n",
    "    os.remove('temp_delme.p')\n",
    "    return model_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make torch deterministic\n",
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "# Create a dataloader for the training\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Load the MNIST test set\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Define the device\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the non-quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNN(nn.Module):\n",
    "    def __init__(self, input_size=28*28, hidden_size_1=100, hidden_size_2=100, output_size=10):\n",
    "        super(FCNN,self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size_1) \n",
    "        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2) \n",
    "        self.linear3 = nn.Linear(hidden_size_2, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters and setting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 28 * 28  # MNIST image size\n",
    "hidden_size_1 = 100\n",
    "hidden_size_2 = 100\n",
    "output_size = 10\n",
    "batch_size = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FCNN(input_size, hidden_size_1, hidden_size_2, output_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, epochs=5):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "MODEL_FILENAME = 'FCNN_ptq.pt'\n",
    "if Path(MODEL_FILENAME).exists():\n",
    "    model.load_state_dict(torch.load(MODEL_FILENAME))\n",
    "    print('Loaded model from disk')\n",
    "else:\n",
    "    train(train_loader, model, epochs=num_epochs)\n",
    "    # Save the model to disk\n",
    "    torch.save(model.state_dict(), MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader,model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            #print('Predicted (Pruned):', predicted.cpu().tolist())\n",
    "            #print('Actual           :', labels.cpu().tolist())\n",
    "            for idx, i in enumerate(outputs):\n",
    "                if torch.argmax(i) == labels[idx]:\n",
    "                    correct +=1\n",
    "                total +=1\n",
    "            #break  # Display predictions for the first batch\n",
    "    print(f'Accuracy: {round(correct/total, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print weights and size of the model before quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights before quantization\n",
      "Parameter containing:\n",
      "tensor([[ 0.0005,  0.0199, -0.0286,  ...,  0.0227,  0.0045,  0.0028],\n",
      "        [-0.0224, -0.0176, -0.0131,  ..., -0.0229, -0.0086, -0.0326],\n",
      "        [ 0.0304,  0.0654,  0.0173,  ...,  0.0302,  0.0517,  0.0586],\n",
      "        ...,\n",
      "        [ 0.0586,  0.0623,  0.0277,  ...,  0.0223,  0.0416,  0.0046],\n",
      "        [-0.0069,  0.0012,  0.0332,  ...,  0.0135,  0.0136,  0.0131],\n",
      "        [ 0.0016, -0.0037, -0.0179,  ...,  0.0186, -0.0307, -0.0106]],\n",
      "       requires_grad=True)\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Print the weights matrix of the model before quantization\n",
    "print('Weights before quantization')\n",
    "print(model.linear1.weight)\n",
    "print(model.linear1.weight.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model before quantization\n",
      "Size (KB): 360.998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "360.998"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Size of the model before quantization')\n",
    "print_size_of_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert min-max observers in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedFCNN(nn.Module):\n",
    "    def __init__(self, input_size=28*28, hidden_size_1=100, hidden_size_2=100, output_size=10):\n",
    "        super(QuantizedFCNN,self).__init__()\n",
    "        self.quant = torch.quantization.QuantStub() #Quantize stub module, before calibration, this is same as an observer,\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size_1) \n",
    "        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2) \n",
    "        self.linear3 = nn.Linear(hidden_size_2, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dequant = torch.quantization.DeQuantStub() #Dequantize stub module, before calibration\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        x = self.quant(x)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizedFCNN(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (linear1): Linear(\n",
       "    in_features=784, out_features=100, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (linear2): Linear(\n",
       "    in_features=100, out_features=100, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (linear3): Linear(\n",
       "    in_features=100, out_features=10, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_q = QuantizedFCNN().to(device)\n",
    "# Copy weights from unquantized model\n",
    "model_q.load_state_dict(model.state_dict())\n",
    "model_q.eval()\n",
    "\n",
    "model_q.qconfig = torch.ao.quantization.default_qconfig # Default qconfig configuration.\n",
    "model_q = torch.ao.quantization.prepare(model_q) # Insert observers\n",
    "model_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibrate the model using the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check statistics of the various layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuantizedFCNN(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (linear1): Linear(\n",
       "    in_features=784, out_features=100, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (linear2): Linear(\n",
       "    in_features=100, out_features=100, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (linear3): Linear(\n",
       "    in_features=100, out_features=10, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Check statistics of the various layers')\n",
    "model_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize the model using the statistics collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.959\n"
     ]
    }
   ],
   "source": [
    "test(test_loader,model_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_q = torch.ao.quantization.convert(model_q) #Apply quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check statistics of the various layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuantizedFCNN(\n",
       "  (quant): Quantize(scale=tensor([0.0256]), zero_point=tensor([17]), dtype=torch.quint8)\n",
       "  (linear1): QuantizedLinear(in_features=784, out_features=100, scale=0.7031587362289429, zero_point=73, qscheme=torch.per_tensor_affine)\n",
       "  (linear2): QuantizedLinear(in_features=100, out_features=100, scale=0.38912928104400635, zero_point=62, qscheme=torch.per_tensor_affine)\n",
       "  (linear3): QuantizedLinear(in_features=100, out_features=10, scale=0.3509964346885681, zero_point=71, qscheme=torch.per_tensor_affine)\n",
       "  (relu): ReLU()\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Check statistics of the various layers')\n",
    "model_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print weights of the model after quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after quantization\n",
      "tensor([[ 0,  4, -6,  ...,  5,  1,  1],\n",
      "        [-5, -4, -3,  ..., -5, -2, -7],\n",
      "        [ 7, 15,  4,  ...,  7, 12, 13],\n",
      "        ...,\n",
      "        [13, 14,  6,  ...,  5,  9,  1],\n",
      "        [-2,  0,  7,  ...,  3,  3,  3],\n",
      "        [ 0, -1, -4,  ...,  4, -7, -2]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "# Print the weights matrix of the model after quantization\n",
    "print('Weights after quantization')\n",
    "print(torch.int_repr(model_q.linear1.weight()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the dequantized weights and the original weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original weights: \n",
      "Parameter containing:\n",
      "tensor([[ 0.0005,  0.0199, -0.0286,  ...,  0.0227,  0.0045,  0.0028],\n",
      "        [-0.0224, -0.0176, -0.0131,  ..., -0.0229, -0.0086, -0.0326],\n",
      "        [ 0.0304,  0.0654,  0.0173,  ...,  0.0302,  0.0517,  0.0586],\n",
      "        ...,\n",
      "        [ 0.0586,  0.0623,  0.0277,  ...,  0.0223,  0.0416,  0.0046],\n",
      "        [-0.0069,  0.0012,  0.0332,  ...,  0.0135,  0.0136,  0.0131],\n",
      "        [ 0.0016, -0.0037, -0.0179,  ...,  0.0186, -0.0307, -0.0106]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Dequantized weights: \n",
      "tensor([[ 0.0000,  0.0180, -0.0269,  ...,  0.0224,  0.0045,  0.0045],\n",
      "        [-0.0224, -0.0180, -0.0135,  ..., -0.0224, -0.0090, -0.0314],\n",
      "        [ 0.0314,  0.0673,  0.0180,  ...,  0.0314,  0.0539,  0.0584],\n",
      "        ...,\n",
      "        [ 0.0584,  0.0628,  0.0269,  ...,  0.0224,  0.0404,  0.0045],\n",
      "        [-0.0090,  0.0000,  0.0314,  ...,  0.0135,  0.0135,  0.0135],\n",
      "        [ 0.0000, -0.0045, -0.0180,  ...,  0.0180, -0.0314, -0.0090]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Original weights: ')\n",
    "print(model.linear1.weight)\n",
    "print('')\n",
    "print(f'Dequantized weights: ')\n",
    "print(torch.dequantize(model_q.linear1.weight()))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.0000e+01, 3.5000e+01, 1.1000e+02, 5.0900e+02, 3.0720e+03,\n",
       "        2.1779e+04, 4.7272e+04, 5.1490e+03, 4.5200e+02, 1.2000e+01]),\n",
       " array([-0.57012123, -0.47764486, -0.38516852, -0.29269215, -0.2002158 ,\n",
       "        -0.10773945, -0.01526309,  0.07721327,  0.16968963,  0.26216596,\n",
       "         0.35464233]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(model.linear1.weight.cpu().detach().numpy().flatten())\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(torch.dequantize(model_q.linear1.weight()).cpu().detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print size and accuracy of the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model before quantization\n",
      "Size (KB): 360.998\n",
      "Size of the model after quantization\n",
      "Size (KB): 95.394\n",
      "Compression Ratio:  4.0\n"
     ]
    }
   ],
   "source": [
    "print('Size of the model before quantization')\n",
    "model_size = print_size_of_model(model)\n",
    "print('Size of the model after quantization')\n",
    "modelq_size = print_size_of_model(model_q)\n",
    "print('Compression Ratio: ',np.round(model_size/modelq_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model before quantization\n",
      "Accuracy: 0.959\n",
      "Testing the model after quantization\n",
      "Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "print('Testing the model before quantization')\n",
    "test(test_loader, model)\n",
    "print('Testing the model after quantization')\n",
    "test(test_loader, model_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
