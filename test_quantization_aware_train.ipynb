{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ref https://github.com/hkproj/quantization-notes/tree/main\n",
    "#Ref https://pytorch.org/docs/stable/quantization-support.html\n",
    "import torch\n",
    "import torchvision.datasets as datasets \n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    model_size = os.path.getsize(\"temp.p\")/1e3\n",
    "    print('Size (KB):', model_size)\n",
    "    os.remove('temp.p')\n",
    "    return model_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make torch deterministic\n",
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "# Create a dataloader for the training\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Load the MNIST test set\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Define the device\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the non-quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedFCNN(nn.Module):\n",
    "    def __init__(self, input_size=28*28, hidden_size_1=100, hidden_size_2=100, output_size=10):\n",
    "        super(QuantizedFCNN,self).__init__()\n",
    "        self.quant = torch.quantization.QuantStub() #Quantize stub module, before calibration, this is same as an observer,\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size_1) \n",
    "        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2) \n",
    "        self.linear3 = nn.Linear(hidden_size_2, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dequant = torch.quantization.DeQuantStub() #Dequantize stub module, before calibration\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        x = self.quant(x)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters and setting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 28 * 28  # MNIST image size\n",
    "hidden_size_1 = 100\n",
    "hidden_size_2 = 100\n",
    "output_size = 10\n",
    "batch_size = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QuantizedFCNN(input_size, hidden_size_1, hidden_size_2, output_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert min-max observers in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizedFCNN(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (linear1): Linear(\n",
       "    in_features=784, out_features=100, bias=True\n",
       "    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (linear2): Linear(\n",
       "    in_features=100, out_features=100, bias=True\n",
       "    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (linear3): Linear(\n",
       "    in_features=100, out_features=10, bias=True\n",
       "    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.qconfig = torch.ao.quantization.default_qconfig # Default qconfig configuration.\n",
    "model.train()\n",
    "modelq = torch.ao.quantization.prepare_qat(model) # Do quantization aware training and output a quantized model\n",
    "modelq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, epochs=5):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            if (i+1) % 500 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [500/6000], Loss: 0.3148\n",
      "Epoch [1/1], Step [1000/6000], Loss: 0.0154\n",
      "Epoch [1/1], Step [1500/6000], Loss: 0.4942\n",
      "Epoch [1/1], Step [2000/6000], Loss: 0.3099\n",
      "Epoch [1/1], Step [2500/6000], Loss: 0.0246\n",
      "Epoch [1/1], Step [3000/6000], Loss: 0.1396\n",
      "Epoch [1/1], Step [3500/6000], Loss: 0.3697\n",
      "Epoch [1/1], Step [4000/6000], Loss: 0.0030\n",
      "Epoch [1/1], Step [4500/6000], Loss: 0.2611\n",
      "Epoch [1/1], Step [5000/6000], Loss: 0.0570\n",
      "Epoch [1/1], Step [5500/6000], Loss: 0.0118\n",
      "Epoch [1/1], Step [6000/6000], Loss: 0.0445\n"
     ]
    }
   ],
   "source": [
    "train(train_loader, modelq, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader,model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            for idx, i in enumerate(outputs):\n",
    "                if torch.argmax(i) == labels[idx]:\n",
    "                    correct +=1\n",
    "                total +=1\n",
    "                \n",
    "    print(f'Accuracy: {round(correct/total, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the collected statistics during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check the statistics of the various layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuantizedFCNN(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=-0.4242129623889923, max_val=2.821486711502075)\n",
       "  )\n",
       "  (linear1): Linear(\n",
       "    in_features=784, out_features=100, bias=True\n",
       "    (weight_fake_quant): MinMaxObserver(min_val=-0.5318018198013306, max_val=0.3377061188220978)\n",
       "    (activation_post_process): MinMaxObserver(min_val=-41.31510925292969, max_val=37.93396759033203)\n",
       "  )\n",
       "  (linear2): Linear(\n",
       "    in_features=100, out_features=100, bias=True\n",
       "    (weight_fake_quant): MinMaxObserver(min_val=-0.44693854451179504, max_val=0.34002432227134705)\n",
       "    (activation_post_process): MinMaxObserver(min_val=-39.279144287109375, max_val=20.906049728393555)\n",
       "  )\n",
       "  (linear3): Linear(\n",
       "    in_features=100, out_features=10, bias=True\n",
       "    (weight_fake_quant): MinMaxObserver(min_val=-0.4316416084766388, max_val=0.2085840106010437)\n",
       "    (activation_post_process): MinMaxObserver(min_val=-33.121334075927734, max_val=22.81952667236328)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Check the statistics of the various layers')\n",
    "modelq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize the model using the statistics collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelq.eval()\n",
    "modelq = torch.ao.quantization.convert(modelq) # Quantize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check statistics of the various layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuantizedFCNN(\n",
       "  (quant): Quantize(scale=tensor([0.0256]), zero_point=tensor([17]), dtype=torch.quint8)\n",
       "  (linear1): QuantizedLinear(in_features=784, out_features=100, scale=0.6240084767341614, zero_point=66, qscheme=torch.per_tensor_affine)\n",
       "  (linear2): QuantizedLinear(in_features=100, out_features=100, scale=0.4738991856575012, zero_point=83, qscheme=torch.per_tensor_affine)\n",
       "  (linear3): QuantizedLinear(in_features=100, out_features=10, scale=0.44047921895980835, zero_point=75, qscheme=torch.per_tensor_affine)\n",
       "  (relu): ReLU()\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Check statistics of the various layers')\n",
    "modelq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print weights and size of the model after quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after quantization\n",
      "tensor([[ 4,  9, -3,  ..., 10,  5,  5],\n",
      "        [-7, -5, -4,  ..., -7, -3, -9],\n",
      "        [-1,  8, -4,  ..., -1,  4,  6],\n",
      "        ...,\n",
      "        [ 7,  8,  0,  ..., -1,  3, -5],\n",
      "        [-2,  0,  8,  ...,  3,  3,  3],\n",
      "        [ 5,  4,  1,  ...,  9, -2,  2]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "# Print the weights matrix of the model after quantization\n",
    "print('Weights after quantization')\n",
    "print(torch.int_repr(modelq.linear1.weight()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.0000e+00, 3.1000e+01, 1.7000e+02, 7.4400e+02, 3.7770e+03,\n",
       "        2.2796e+04, 4.3806e+04, 6.3720e+03, 6.5900e+02, 4.3000e+01]),\n",
       " array([-127. , -106.3,  -85.6,  -64.9,  -44.2,  -23.5,   -2.8,   17.9,\n",
       "          38.6,   59.3,   80. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAGdCAYAAADwjmIIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAiWUlEQVR4nO3de1BU993H8Q+gu4i6i1cIEcWMrUq8VVTcNMk8NsRtSjq1YgdbxxCjSbVoo1gvNA5qJilWp1FTb8lkGpxpHC9/JK0SsQ5GM40bLyiNGnGSVIsdXCATYZVRQDjPHy0nbsVEVFz58X7N7CSc893Db/cM8s66exJmWZYlAAAAw4SHegEAAACtgcgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYKQOoV5AKDU2NqqsrExdu3ZVWFhYqJcDAABugWVZunTpkuLi4hQefvPXa9p15JSVlSk+Pj7UywAAALfh/Pnz6tOnz033t+vI6dq1q6T/PEkulyvEqwEAALciEAgoPj7e/j1+M+06cpr+isrlchE5AAC0Md/2VhPeeAwAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACN1CPUCAABfS1icH+oltNi5FamhXgLQLF7JAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAke4oclasWKGwsDDNnTvX3nb16lVlZmaqR48e6tKli9LS0lReXh50v9LSUqWmpioqKkq9e/fWggULdO3ataCZ/fv3a+TIkXI6nRowYIDy8vJu+P7r169XQkKCIiMjlZycrMOHD9/JwwEAAAa57cg5cuSI3njjDQ0bNixo+7x587Rz507t2LFDBw4cUFlZmSZOnGjvb2hoUGpqqurq6nTw4EFt3rxZeXl5ysnJsWfOnj2r1NRUjRs3TsXFxZo7d65mzJihPXv22DPbtm1TVlaWli5dqmPHjmn48OHyer2qqKi43YcEAAAMEmZZltXSO12+fFkjR47Uhg0b9Morr2jEiBFas2aNqqur1atXL23ZskWTJk2SJJWUlGjw4MHy+XwaO3asdu/eraefflplZWWKiYmRJG3atEmLFi1SZWWlHA6HFi1apPz8fJ08edL+npMnT1ZVVZUKCgokScnJyRo9erTWrVsnSWpsbFR8fLzmzJmjxYsX39LjCAQCcrvdqq6ulsvlaunTAAB3XcLi/FAvocXOrUgN9RLQztzq7+/beiUnMzNTqampSklJCdpeVFSk+vr6oO2DBg1S37595fP5JEk+n09Dhw61A0eSvF6vAoGATp06Zc/877G9Xq99jLq6OhUVFQXNhIeHKyUlxZ5pTm1trQKBQNANAACYqUNL77B161YdO3ZMR44cuWGf3++Xw+FQdHR00PaYmBj5/X575vrAadrftO+bZgKBgK5cuaKLFy+qoaGh2ZmSkpKbrj03N1fLly+/tQcKAADatBa9knP+/Hm9+OKLeueddxQZGdlaa2o12dnZqq6utm/nz58P9ZIAAEAraVHkFBUVqaKiQiNHjlSHDh3UoUMHHThwQK+//ro6dOigmJgY1dXVqaqqKuh+5eXlio2NlSTFxsbe8Gmrpq+/bcblcqlTp07q2bOnIiIimp1pOkZznE6nXC5X0A0AAJipRZHzxBNP6MSJEyouLrZvo0aN0pQpU+x/79ixowoLC+37nDlzRqWlpfJ4PJIkj8ejEydOBH0Kau/evXK5XEpMTLRnrj9G00zTMRwOh5KSkoJmGhsbVVhYaM8AAID2rUXvyenatauGDBkStK1z587q0aOHvX369OnKyspS9+7d5XK5NGfOHHk8Ho0dO1aSNH78eCUmJmrq1KlauXKl/H6/lixZoszMTDmdTknSzJkztW7dOi1cuFDPPfec9u3bp+3btys//+tPHWRlZSkjI0OjRo3SmDFjtGbNGtXU1GjatGl39IQAAAAztPiNx99m9erVCg8PV1pammpra+X1erVhwwZ7f0REhHbt2qVZs2bJ4/Goc+fOysjI0Msvv2zP9O/fX/n5+Zo3b57Wrl2rPn366K233pLX67Vn0tPTVVlZqZycHPn9fo0YMUIFBQU3vBkZAAC0T7d1nRxTcJ0cAPcbrpMDfLtWvU4OAADA/Y7IAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABipRZGzceNGDRs2TC6XSy6XSx6PR7t377b3X716VZmZmerRo4e6dOmitLQ0lZeXBx2jtLRUqampioqKUu/evbVgwQJdu3YtaGb//v0aOXKknE6nBgwYoLy8vBvWsn79eiUkJCgyMlLJyck6fPhwSx4KAAAwXIsip0+fPlqxYoWKiop09OhR/eAHP9BPfvITnTp1SpI0b9487dy5Uzt27NCBAwdUVlamiRMn2vdvaGhQamqq6urqdPDgQW3evFl5eXnKycmxZ86ePavU1FSNGzdOxcXFmjt3rmbMmKE9e/bYM9u2bVNWVpaWLl2qY8eOafjw4fJ6vaqoqLjT5wMAABgizLIs604O0L17d61atUqTJk1Sr169tGXLFk2aNEmSVFJSosGDB8vn82ns2LHavXu3nn76aZWVlSkmJkaStGnTJi1atEiVlZVyOBxatGiR8vPzdfLkSft7TJ48WVVVVSooKJAkJScna/To0Vq3bp0kqbGxUfHx8ZozZ44WL158y2sPBAJyu92qrq6Wy+W6k6cBAO6KhMX5oV5Ci51bkRrqJaCdudXf37f9npyGhgZt3bpVNTU18ng8KioqUn19vVJSUuyZQYMGqW/fvvL5fJIkn8+noUOH2oEjSV6vV4FAwH41yOfzBR2jaabpGHV1dSoqKgqaCQ8PV0pKij1zM7W1tQoEAkE3AABgphZHzokTJ9SlSxc5nU7NnDlT7777rhITE+X3++VwOBQdHR00HxMTI7/fL0ny+/1BgdO0v2nfN80EAgFduXJFX375pRoaGpqdaTrGzeTm5srtdtu3+Pj4lj58AADQRrQ4cgYOHKji4mIdOnRIs2bNUkZGhj799NPWWNtdl52drerqavt2/vz5UC8JAAC0kg4tvYPD4dCAAQMkSUlJSTpy5IjWrl2r9PR01dXVqaqqKujVnPLycsXGxkqSYmNjb/gUVNOnr66f+d9PZJWXl8vlcqlTp06KiIhQREREszNNx7gZp9Mpp9PZ0ocMAADaoDu+Tk5jY6Nqa2uVlJSkjh07qrCw0N535swZlZaWyuPxSJI8Ho9OnDgR9CmovXv3yuVyKTEx0Z65/hhNM03HcDgcSkpKCpppbGxUYWGhPQMAANCiV3Kys7P11FNPqW/fvrp06ZK2bNmi/fv3a8+ePXK73Zo+fbqysrLUvXt3uVwuzZkzRx6PR2PHjpUkjR8/XomJiZo6dapWrlwpv9+vJUuWKDMz036FZebMmVq3bp0WLlyo5557Tvv27dP27duVn//1Jw6ysrKUkZGhUaNGacyYMVqzZo1qamo0bdq0u/jUAACAtqxFkVNRUaFnnnlGFy5ckNvt1rBhw7Rnzx49+eSTkqTVq1crPDxcaWlpqq2tldfr1YYNG+z7R0REaNeuXZo1a5Y8Ho86d+6sjIwMvfzyy/ZM//79lZ+fr3nz5mnt2rXq06eP3nrrLXm9XnsmPT1dlZWVysnJkd/v14gRI1RQUHDDm5EBAED7dcfXyWnLuE4OgPsN18kBvl2rXycHAADgfkbkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASB1CvQAAaC0Ji/NDvQQAIcQrOQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASC2KnNzcXI0ePVpdu3ZV7969NWHCBJ05cyZo5urVq8rMzFSPHj3UpUsXpaWlqby8PGimtLRUqampioqKUu/evbVgwQJdu3YtaGb//v0aOXKknE6nBgwYoLy8vBvWs379eiUkJCgyMlLJyck6fPhwSx4OAAAwWIsi58CBA8rMzNTHH3+svXv3qr6+XuPHj1dNTY09M2/ePO3cuVM7duzQgQMHVFZWpokTJ9r7GxoalJqaqrq6Oh08eFCbN29WXl6ecnJy7JmzZ88qNTVV48aNU3FxsebOnasZM2Zoz5499sy2bduUlZWlpUuX6tixYxo+fLi8Xq8qKiru5PkAAACGCLMsy7rdO1dWVqp37946cOCAHn/8cVVXV6tXr17asmWLJk2aJEkqKSnR4MGD5fP5NHbsWO3evVtPP/20ysrKFBMTI0natGmTFi1apMrKSjkcDi1atEj5+fk6efKk/b0mT56sqqoqFRQUSJKSk5M1evRorVu3TpLU2Nio+Ph4zZkzR4sXL76l9QcCAbndblVXV8vlct3u0wDgPpWwOD/US2gXzq1IDfUS0M7c6u/vO3pPTnV1tSSpe/fukqSioiLV19crJSXFnhk0aJD69u0rn88nSfL5fBo6dKgdOJLk9XoVCAR06tQpe+b6YzTNNB2jrq5ORUVFQTPh4eFKSUmxZwAAQPvW4Xbv2NjYqLlz5+r73/++hgwZIkny+/1yOByKjo4Omo2JiZHf77dnrg+cpv1N+75pJhAI6MqVK7p48aIaGhqanSkpKbnpmmtra1VbW2t/HQgEWvCIAQBAW3Lbr+RkZmbq5MmT2rp1691cT6vKzc2V2+22b/Hx8aFeEgAAaCW3FTmzZ8/Wrl279MEHH6hPnz729tjYWNXV1amqqipovry8XLGxsfbM/37aqunrb5txuVzq1KmTevbsqYiIiGZnmo7RnOzsbFVXV9u38+fPt+yBAwCANqNFkWNZlmbPnq13331X+/btU//+/YP2JyUlqWPHjiosLLS3nTlzRqWlpfJ4PJIkj8ejEydOBH0Kau/evXK5XEpMTLRnrj9G00zTMRwOh5KSkoJmGhsbVVhYaM80x+l0yuVyBd0AAICZWvSenMzMTG3ZskV/+ctf1LVrV/s9NG63W506dZLb7db06dOVlZWl7t27y+Vyac6cOfJ4PBo7dqwkafz48UpMTNTUqVO1cuVK+f1+LVmyRJmZmXI6nZKkmTNnat26dVq4cKGee+457du3T9u3b1d+/teflMjKylJGRoZGjRqlMWPGaM2aNaqpqdG0adPu1nMDAADasBZFzsaNGyVJ//d//xe0/e2339azzz4rSVq9erXCw8OVlpam2tpaeb1ebdiwwZ6NiIjQrl27NGvWLHk8HnXu3FkZGRl6+eWX7Zn+/fsrPz9f8+bN09q1a9WnTx+99dZb8nq99kx6eroqKyuVk5Mjv9+vESNGqKCg4IY3IwMAgPbpjq6T09ZxnRzAbFwn597gOjm41+7JdXIAAADuV0QOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMFKLI+fDDz/Uj3/8Y8XFxSksLEzvvfde0H7LspSTk6MHHnhAnTp1UkpKij777LOgma+++kpTpkyRy+VSdHS0pk+frsuXLwfNfPLJJ3rssccUGRmp+Ph4rVy58oa17NixQ4MGDVJkZKSGDh2q999/v6UPBwAAGKrFkVNTU6Phw4dr/fr1ze5fuXKlXn/9dW3atEmHDh1S586d5fV6dfXqVXtmypQpOnXqlPbu3atdu3bpww8/1AsvvGDvDwQCGj9+vPr166eioiKtWrVKy5Yt05tvvmnPHDx4UD//+c81ffp0HT9+XBMmTNCECRN08uTJlj4kAABgoDDLsqzbvnNYmN59911NmDBB0n9exYmLi9P8+fP1m9/8RpJUXV2tmJgY5eXlafLkyTp9+rQSExN15MgRjRo1SpJUUFCgH/3oR/r3v/+tuLg4bdy4US+99JL8fr8cDockafHixXrvvfdUUlIiSUpPT1dNTY127dplr2fs2LEaMWKENm3adEvrDwQCcrvdqq6ulsvlut2nAcB9KmFxfqiX0C6cW5Ea6iWgnbnV39939T05Z8+eld/vV0pKir3N7XYrOTlZPp9PkuTz+RQdHW0HjiSlpKQoPDxchw4dsmcef/xxO3Akyev16syZM7p48aI9c/33aZpp+j7Nqa2tVSAQCLoBAAAz3dXI8fv9kqSYmJig7TExMfY+v9+v3r17B+3v0KGDunfvHjTT3DGu/x43m2na35zc3Fy53W77Fh8f39KHCAAA2oh29emq7OxsVVdX27fz58+HekkAAKCV3NXIiY2NlSSVl5cHbS8vL7f3xcbGqqKiImj/tWvX9NVXXwXNNHeM67/HzWaa9jfH6XTK5XIF3QAAgJnuauT0799fsbGxKiwstLcFAgEdOnRIHo9HkuTxeFRVVaWioiJ7Zt++fWpsbFRycrI98+GHH6q+vt6e2bt3rwYOHKhu3brZM9d/n6aZpu8DAADatxZHzuXLl1VcXKzi4mJJ/3mzcXFxsUpLSxUWFqa5c+fqlVde0V//+ledOHFCzzzzjOLi4uxPYA0ePFg//OEP9fzzz+vw4cP66KOPNHv2bE2ePFlxcXGSpF/84hdyOByaPn26Tp06pW3btmnt2rXKysqy1/Hiiy+qoKBAf/jDH1RSUqJly5bp6NGjmj179p0/KwAAoM3r0NI7HD16VOPGjbO/bgqPjIwM5eXlaeHChaqpqdELL7ygqqoqPfrooyooKFBkZKR9n3feeUezZ8/WE088ofDwcKWlpen111+397vdbv3tb39TZmamkpKS1LNnT+Xk5ARdS+eRRx7Rli1btGTJEv32t7/Vd77zHb333nsaMmTIbT0RAADALHd0nZy2juvkAGbjOjn3BtfJwb0WkuvkAAAA3C+IHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGCkDqFeAACgbUtYnB/qJbTYuRWpoV4C7gFeyQEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYqUOoFwCgbUhYnB/qJQBAi/BKDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACN1CPUCgPYmYXF+qJcAAO0CkQMAaHfa4n9snFuRGuoltDlt/q+r1q9fr4SEBEVGRio5OVmHDx8O9ZIAAMB9oE1HzrZt25SVlaWlS5fq2LFjGj58uLxeryoqKkK9NAAAEGJtOnJee+01Pf/885o2bZoSExO1adMmRUVF6U9/+lOolwYAAEKszb4np66uTkVFRcrOzra3hYeHKyUlRT6fr9n71NbWqra21v66urpakhQIBFp3sWg1Q5buCfUSAOCe4HfV15qeC8uyvnGuzUbOl19+qYaGBsXExARtj4mJUUlJSbP3yc3N1fLly2/YHh8f3yprBADgbnGvCfUK7j+XLl2S2+2+6f42Gzm3Izs7W1lZWfbXjY2N+uqrr9SjRw+FhYWFcGVtVyAQUHx8vM6fPy+XyxXq5eAWcM7aHs5Z28M5a12WZenSpUuKi4v7xrk2Gzk9e/ZURESEysvLg7aXl5crNja22fs4nU45nc6gbdHR0a21xHbF5XLxg9zGcM7aHs5Z28M5az3f9ApOkzb7xmOHw6GkpCQVFhba2xobG1VYWCiPxxPClQEAgPtBm30lR5KysrKUkZGhUaNGacyYMVqzZo1qamo0bdq0UC8NAACEWJuOnPT0dFVWVionJ0d+v18jRoxQQUHBDW9GRutxOp1aunTpDX8NiPsX56zt4Zy1PZyz+0OY9W2fvwIAAGiD2ux7cgAAAL4JkQMAAIxE5AAAACMROQAAwEhEDm7Jq6++qkceeURRUVE3vYBiaWmpUlNTFRUVpd69e2vBggW6du1a0Mz+/fs1cuRIOZ1ODRgwQHl5ea2/eNgSEhIUFhYWdFuxYkXQzCeffKLHHntMkZGRio+P18qVK0O0WkjS+vXrlZCQoMjISCUnJ+vw4cOhXhL+a9myZTf8PA0aNMjef/XqVWVmZqpHjx7q0qWL0tLSbriALVoXkYNbUldXp5/97GeaNWtWs/sbGhqUmpqquro6HTx4UJs3b1ZeXp5ycnLsmbNnzyo1NVXjxo1TcXGx5s6dqxkzZmjPHv4nm/fSyy+/rAsXLti3OXPm2PsCgYDGjx+vfv36qaioSKtWrdKyZcv05ptvhnDF7de2bduUlZWlpUuX6tixYxo+fLi8Xq8qKipCvTT818MPPxz08/T3v//d3jdv3jzt3LlTO3bs0IEDB1RWVqaJEyeGcLXtkAW0wNtvv2253e4btr///vtWeHi45ff77W0bN260XC6XVVtba1mWZS1cuNB6+OGHg+6Xnp5ueb3eVl0zvtavXz9r9erVN92/YcMGq1u3bvY5syzLWrRokTVw4MB7sDr8rzFjxliZmZn21w0NDVZcXJyVm5sbwlWhydKlS63hw4c3u6+qqsrq2LGjtWPHDnvb6dOnLUmWz+e7RysEr+TgrvD5fBo6dGjQhRi9Xq8CgYBOnTplz6SkpATdz+v1yufz3dO1tncrVqxQjx499L3vfU+rVq0K+itFn8+nxx9/XA6Hw97m9Xp15swZXbx4MRTLbbfq6upUVFQU9DMTHh6ulJQUfmbuI5999pni4uL00EMPacqUKSotLZUkFRUVqb6+Puj8DRo0SH379uX83UNt+orHuH/4/f4brjTd9LXf7//GmUAgoCtXrqhTp073ZrHt2K9//WuNHDlS3bt318GDB5Wdna0LFy7otddek/Sfc9S/f/+g+1x/Hrt163bP19xeffnll2poaGj2Z6akpCREq8L1kpOTlZeXp4EDB+rChQtavny5HnvsMZ08eVJ+v18Oh+OG9zDGxMTYfyai9RE57djixYv1+9///htnTp8+HfRGOtx/WnIes7Ky7G3Dhg2Tw+HQL3/5S+Xm5nL5eaCFnnrqKfvfhw0bpuTkZPXr10/bt2/nP9ruE0ROOzZ//nw9++yz3zjz0EMP3dKxYmNjb/jUR9OnCGJjY+1//u8nC8rLy+VyufgD4Q7cyXlMTk7WtWvXdO7cOQ0cOPCm50j6+jzi3ujZs6ciIiKaPR+ci/tTdHS0vvvd7+rzzz/Xk08+qbq6OlVVVQW9msP5u7eInHasV69e6tWr1105lsfj0auvvqqKigr17t1bkrR37165XC4lJibaM++//37Q/fbu3SuPx3NX1tBe3cl5LC4uVnh4uH3OPB6PXnrpJdXX16tjx46S/nOOBg4cyF9V3WMOh0NJSUkqLCzUhAkTJEmNjY0qLCzU7NmzQ7s4NOvy5cv64osvNHXqVCUlJaljx44qLCxUWlqaJOnMmTMqLS3lz7x7KdTvfEbb8K9//cs6fvy4tXz5cqtLly7W8ePHrePHj1uXLl2yLMuyrl27Zg0ZMsQaP368VVxcbBUUFFi9evWysrOz7WP885//tKKioqwFCxZYp0+fttavX29FRERYBQUFoXpY7crBgwet1atXW8XFxdYXX3xh/fnPf7Z69eplPfPMM/ZMVVWVFRMTY02dOtU6efKktXXrVisqKsp64403Qrjy9mvr1q2W0+m08vLyrE8//dR64YUXrOjo6KBPMSJ05s+fb+3fv986e/as9dFHH1kpKSlWz549rYqKCsuyLGvmzJlW3759rX379llHjx61PB6P5fF4Qrzq9oXIwS3JyMiwJN1w++CDD+yZc+fOWU899ZTVqVMnq2fPntb8+fOt+vr6oON88MEH1ogRIyyHw2E99NBD1ttvv31vH0g7VlRUZCUnJ1tut9uKjIy0Bg8ebP3ud7+zrl69GjT3j3/8w3r00Uctp9NpPfjgg9aKFStCtGJYlmX98Y9/tPr27Ws5HA5rzJgx1scffxzqJeG/0tPTrQceeMByOBzWgw8+aKWnp1uff/65vf/KlSvWr371K6tbt25WVFSU9dOf/tS6cOFCCFfc/oRZlmWF9KUkAACAVsB1cgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEb6f5HAzozBjFfzAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(torch.int_repr(modelq.linear1.weight()).cpu().detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model after quantization\n",
      "Accuracy: 0.952\n"
     ]
    }
   ],
   "source": [
    "print('Testing the model after quantization')\n",
    "test(test_loader,modelq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model after quantization\n",
      "Size (KB): 95.158\n"
     ]
    }
   ],
   "source": [
    "print('Size of the model after quantization')\n",
    "modelq_size = print_size_of_model(modelq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
